kind: live
title: llama_factory

# other files from https://github.com/hiyouga/LLaMA-Factory

defaults:
  life_span: 5d

images:
  llama_factory:
    ref: image:$[[ project.id ]]:v1
    dockerfile: $[[ flow.workspace ]]/docker/docker-cuda/Dockerfile
    context: $[[ flow.workspace ]]/

volumes:
  hf_cache:
    remote: storage:$[[ flow.project_id ]]/hf_cache
    mount: /root/.cache/huggingface
    local: hf_cache
  data:
    remote: storage:$[[ flow.project_id ]]/data
    mount: /app/data
    local: data
  output:
    remote: storage:$[[ flow.project_id ]]/output
    mount: /app/saves
    local: output

jobs:
  llama_factory_webui:
    image: ${{ images.llama_factory.ref }}
    name: llama-factory
    preset: a100x1
    http_port: "7860"
    detach: true
    browse: true
    env:
      HF_TOKEN: secret:HF_TOKEN
    volumes:
      - ${{ upload(volumes.data).ref_rw }}
      - ${{ volumes.output.ref_rw }}
      - ${{ volumes.hf_cache.ref_rw }}
    cmd: bash -c "cd /app && llamafactory-cli webui"

  vllm:
    image: vllm/vllm-openai:v0.6.4.post1
    name: vllm
    preset: a100x1
    detach: true
    http_port: "8000"
    volumes:
      - ${{ volumes.output.ref_rw }}
    env:
      HF_TOKEN: secret:HF_TOKEN
    cmd: >
      --model meta-llama/Meta-Llama-3.1-8B-Instruct
      --tokenizer meta-llama/Meta-Llama-3.1-8B-Instruct
      --enable-lora
      --lora-modules trained_lora=/app/saves/Llama-3.1-8B-Instruct/lora/train_2024-11-26-16-47-45
      --dtype=half